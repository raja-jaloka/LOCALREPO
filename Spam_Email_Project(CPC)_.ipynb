{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7UIsJ3p7Cu9AGs5J7I+CR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jaloka/LOCALREPO/blob/main/Spam_Email_Project(CPC)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPC PROJECT-1 SPAM EMAIL DETECTION/CLASSIFIER"
      ],
      "metadata": {
        "id": "L-gfNaR17770"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading & Understanding the Data"
      ],
      "metadata": {
        "id": "-Q21ZhWGjpPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Loading Data\n",
        "import pandas as pd #will be used in this project to access the csv dataset used to train our model\n",
        "import numpy as np #will be used for array operations on the accessed dataset\n",
        "from google.colab import files #one of the multiple methods to upload a file saved on the local system onto the google collab notebook\n",
        "uploaded=files.upload() #Notice a pop-up opens when this line is run asking to select the file\n",
        "import io #input output library\n",
        "data=pd.read_csv(io.BytesIO(uploaded['spam_ham_dataset (1).csv']), encoding='latin-1') #uploaded[...] is a dictionary of the raw binary code which using the io.BytesIO is converted to be read in a file format but we faced a problem here because of certain encodings in the file so we used the encoding='latin-1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "ymEoLX-YfIYP",
        "outputId": "a7c0420d-f13d-4e99-ded7-78eec1668dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a579bd3c-2892-4652-9511-aa778e75208e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a579bd3c-2892-4652-9511-aa778e75208e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving spam_ham_dataset.csv to spam_ham_dataset (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding Data\n",
        "print(data.columns)\n",
        "print(data.info)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Tvdsi4ibcz",
        "outputId": "ecb0b18b-a2db-4c6d-cdec-42f99d9c440c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object')\n",
            "<bound method DataFrame.info of       Unnamed: 0 label                                               text  \\\n",
            "0            605   ham  Subject: enron methanol ; meter # : 988291\\nth...   \n",
            "1           2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...   \n",
            "2           3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...   \n",
            "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
            "4           2030   ham  Subject: re : indian springs\\nthis deal is to ...   \n",
            "...          ...   ...                                                ...   \n",
            "5166        1518   ham  Subject: put the 10 on the ft\\nthe transport v...   \n",
            "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\nhpl ...   \n",
            "5168        2933   ham  Subject: calpine daily gas nomination\\n>\\n>\\nj...   \n",
            "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
            "5170        4807  spam  Subject: important online banking alert\\ndear ...   \n",
            "\n",
            "      label_num  \n",
            "0             0  \n",
            "1             0  \n",
            "2             0  \n",
            "3             1  \n",
            "4             0  \n",
            "...         ...  \n",
            "5166          0  \n",
            "5167          0  \n",
            "5168          0  \n",
            "5169          0  \n",
            "5170          1  \n",
            "\n",
            "[5171 rows x 4 columns]>\n",
            "   Unnamed: 0 label                                               text  \\\n",
            "0         605   ham  Subject: enron methanol ; meter # : 988291\\nth...   \n",
            "1        2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...   \n",
            "2        3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...   \n",
            "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
            "4        2030   ham  Subject: re : indian springs\\nthis deal is to ...   \n",
            "\n",
            "   label_num  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          1  \n",
            "4          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "ftukMwoljmlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning the data\n",
        "'''from sklearn.impute import SimpleImputer\n",
        "imputer=SimpleImputer(strategy='mean')\n",
        "imputed_data=imputer.fit_transform(data)\n",
        "print(data)'''\n",
        "#Updates 4/10/25 : currently working on finding another method to correct the ValueError:Cannot use mean strategy with non-numeric data:could not convert string to float: 'ham'\n",
        "#Updates 4/10/25: Learning more about using OneHotEncoder instead\n",
        "data.duplicated() #Therefore all the rows are unique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "6LH2mBdfi9BC",
        "outputId": "131de799-e89c-4ed7-bed3-8ed1f12729ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       False\n",
              "1       False\n",
              "2       False\n",
              "3       False\n",
              "4       False\n",
              "        ...  \n",
              "5166    False\n",
              "5167    False\n",
              "5168    False\n",
              "5169    False\n",
              "5170    False\n",
              "Length: 5171, dtype: bool"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> bool</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we segregate the columns into numerical and categorical types---dssh.csv(our training dataset) won't specifically require for this segregation this however is done keeping in mind all sorts of datasets\n",
        "cat_col=[col for col in data.columns if data[col].dtype=='object']\n",
        "num_col=[col for col in data.columns if data[col].dtype!='object']\n",
        "print(cat_col)\n",
        "print(num_col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xur-iOlTjlfI",
        "outputId": "32e3c6e4-a0f8-48d9-a762-2d41520bdc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['label', 'text']\n",
            "['Unnamed: 0', 'label_num']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2blCjWzTS3U",
        "outputId": "ce32da07-e3f6-40f3-caa8-8cf0b727b539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we access the text and perform removing stopwords, punctuations & normalization of the data\n",
        "from nltk.tokenize import word_tokenize\n",
        "data['tokens']=data['text'].apply(word_tokenize) #.apply tokenizes each row of text\n",
        "#print(data['tokens']) #successfully tokenized\n",
        "\n",
        "#now we normalize the data\n",
        "import string\n",
        "data['tokens'] = [[str(tok).lower() for tok in token_list] for token_list in data['tokens']]\n",
        "#print(data['tokens']) #normalizing is to convert all characters in the token_list to lowercase\n",
        "\n",
        "#now we remove punctuations\n",
        "import string\n",
        "data['tokens'] = [[tok for tok in token_list if tok not in string.punctuation] for token_list in data['tokens']]\n",
        "#print(data['tokens'])\n",
        "\n",
        "#now we remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=set(stopwords.words('english')) #initialised stop_word to a set of english words in stopword. set was used since set has a fundamental property of not containing duplicate entries\n",
        "data['tokens']=[[token for token in token_list if token not in stop_words]for token_list in data['tokens']]\n",
        "print(data['tokens'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd8aidJv1AQB",
        "outputId": "993b8192-a4f4-42e2-c343-e46c11e93f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       [subject, enron, methanol, meter, 988291, foll...\n",
            "1       [subject, hpl, nom, january, 9, 2001, see, att...\n",
            "2       [subject, neon, retreat, ho, ho, ho, around, w...\n",
            "3       [subject, photoshop, windows, office, cheap, m...\n",
            "4       [subject, indian, springs, deal, book, teco, p...\n",
            "                              ...                        \n",
            "5166    [subject, put, 10, ft, transport, volumes, dec...\n",
            "5167    [subject, 3, 4, 2000, following, noms, hpl, ta...\n",
            "5168    [subject, calpine, daily, gas, nomination, jul...\n",
            "5169    [subject, industrial, worksheets, august, 2000...\n",
            "5170    [subject, important, online, banking, alert, d...\n",
            "Name: tokens, Length: 5171, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LEMMATIZATION OF THE DATA\n",
        "'''We used Lemmatization here because lemmatization converts a token into it's\n",
        "grammatically correct lemma/reduced form'''\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "data[\"tokens\"]=[[token.lemma_ for token in nlp(' '.join(token_list))] for token_list in data['tokens']]\n",
        "'''when you process text with spacy by passing the npl(text) function\n",
        "the text is tokenized creating a doc object which is passed through several\n",
        "pipeline components which include:\n",
        "tokenizer\n",
        "tagger=assigns part of speech tags to tokens\n",
        "lemmatizer=converts tokens to their base forms\n",
        "parser=analyzes grammatical dependencies\n",
        "entity recognizer=identifies named entities(like people or places)\n",
        "Text categorizer=assigns a label to the text\n",
        "custom components= user defined functions for extra processing\n",
        "'''\n",
        "print(data['tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVDN9qflDMTi",
        "outputId": "ead144b6-caca-489f-ae2a-f0db7a967a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       [subject, enron, methanol, meter, 988291, foll...\n",
            "1       [subject, hpl, nom, january, 9, 2001, see, att...\n",
            "2       [subject, neon, retreat, ho, ho, ho, around, w...\n",
            "3       [subject, photoshop, window, office, cheap, ma...\n",
            "4       [subject, indian, springs, deal, book, teco, p...\n",
            "                              ...                        \n",
            "5166    [subject, put, 10, ft, transport, volume, decr...\n",
            "5167    [subject, 3, 4, 2000, follow, nom, hpl, take, ...\n",
            "5168    [subject, calpine, daily, gas, nomination, jul...\n",
            "5169    [subject, industrial, worksheet, august, 2000,...\n",
            "5170    [subject, important, online, banking, alert, d...\n",
            "Name: tokens, Length: 5171, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "4Ww1EfI_YMPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer=CountVectorizer()\n",
        "#Now to create a single sparse matrix of all the tokens in the data[tokens]\n",
        "corpus= [\" \".join(tokens) for tokens in data[\"tokens\"]]\n",
        "X=vectorizer.fit_transform(corpus) #X is our ML input + produces a document-term matrix(features per document) as a sparse matrix\n",
        "#.fit learns all the parameters corresponding to the data\n",
        "#.fit_transform learns about all the parameters of the given input and then transforms it to a sparse matrix(to be used on training data)\n",
        "#.transform directly transforms the given input to a sparse matrix (doc) based on the fit of the training data, this is used on test data\n",
        "vectorizer.get_feature_names_out()\n",
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkqF7xnLYK1M",
        "outputId": "aa5c7b8c-c77f-48c8-b49b-f1f4cc8a2c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN TEST SPLIT"
      ],
      "metadata": {
        "id": "m9t-_thoWmjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Y= data['label_num']\n",
        "X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=42)\n",
        "# test_Size=0.2 means that 20% of the data is considered as test cases and the rest are used to train the model\n",
        "#random_state=num indicates that as the code snippet as run the order of the train or test datas remain the same"
      ],
      "metadata": {
        "id": "sENQQrf8WkPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL TRAINING\n",
        "\n",
        "\n",
        "1.   Logistic Regression\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1W_xpfoQhz8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model1=LogisticRegression()\n",
        "model1.fit(X_train,Y_train)\n",
        "model1.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"The accuracy score is: \",end=' ')\n",
        "print(accuracy_score(Y_test,model1.predict(X_test)))\n",
        "\n",
        "from sklearn.metrics import recall_score #tp/(tp+fn)\n",
        "print(\"The recall scores are: \",end='=')\n",
        "print(recall_score(Y_test,model1.predict(X_test),average='binary',pos_label=1),end='---')\n",
        "print(recall_score(Y_test,model1.predict(X_test),average='binary',pos_label=0))\n",
        "\n",
        "from sklearn.metrics import precision_score #tp/(tp+fp)\n",
        "print('The precision scores are: ',end='=')\n",
        "print(precision_score(Y_test,model1.predict(X_test),average='binary',pos_label=1),end='---')\n",
        "print(precision_score(Y_test,model1.predict(X_test),average='binary',pos_label=0))\n",
        "\n",
        "from sklearn.metrics import f1_score #2*tp/(2*tp+fp+fn)\n",
        "print(\"The f1_scores are: \",end='=')\n",
        "print(f1_score(Y_test,model1.predict(X_test),average='binary',pos_label=1),end='---')\n",
        "print(f1_score(Y_test,model1.predict(X_test),average='binary',pos_label=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2VT5RV0KhznZ",
        "outputId": "b7509f00-6fa3-42e5-e460-ef648d09de4a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy score is:  0.9816425120772947\n",
            "The recall scores are: =0.9795221843003413---0.9824797843665768\n",
            "The precision scores are: =0.9566666666666667---0.9918367346938776\n",
            "The f1_scores are: =0.9679595278246206---0.987136086662153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment"
      ],
      "metadata": {
        "id": "EHrZpeyi0aAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit-jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWkNyxpVBoz5",
        "outputId": "ea732efd-234c-471c-938e-838d6a69dcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit-jupyter\n",
            "  Downloading streamlit_jupyter-0.3.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.12/dist-packages (from streamlit-jupyter) (1.8.13)\n",
            "Collecting ipywidgets>=8 (from streamlit-jupyter)\n",
            "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting jupyter (from streamlit-jupyter)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting stqdm (from streamlit-jupyter)\n",
            "  Downloading stqdm-0.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: streamlit>=1.19 in /usr/local/lib/python3.12/dist-packages (from streamlit-jupyter) (1.51.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from streamlit-jupyter) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from streamlit-jupyter) (4.67.1)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8->streamlit-jupyter)\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8->streamlit-jupyter) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8->streamlit-jupyter) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8->streamlit-jupyter)\n",
            "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8->streamlit-jupyter) (3.0.15)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.19->streamlit-jupyter) (6.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter->streamlit-jupyter) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter->streamlit-jupyter) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter->streamlit-jupyter) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter->streamlit-jupyter) (6.17.1)\n",
            "Collecting jupyterlab (from jupyter->streamlit-jupyter)\n",
            "  Downloading jupyterlab-4.4.10-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (2.10.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.19->streamlit-jupyter) (4.0.12)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.19->streamlit-jupyter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.19->streamlit-jupyter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.19->streamlit-jupyter) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.19->streamlit-jupyter) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.19->streamlit-jupyter) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.19->streamlit-jupyter) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.19->streamlit-jupyter) (2025.10.5)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->streamlit-jupyter) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->streamlit-jupyter) (7.4.9)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->streamlit-jupyter) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->streamlit-jupyter) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->streamlit-jupyter) (26.2.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->streamlit-jupyter) (5.9.1)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->streamlit-jupyter)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->streamlit-jupyter) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->streamlit-jupyter)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->streamlit-jupyter) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->streamlit-jupyter)\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->streamlit-jupyter) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->streamlit-jupyter) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->streamlit-jupyter) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->streamlit-jupyter) (25.1.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->streamlit-jupyter) (0.2.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->streamlit-jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->streamlit-jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->streamlit-jupyter) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->streamlit-jupyter) (1.3.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->streamlit-jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->streamlit-jupyter) (1.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.19->streamlit-jupyter) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->streamlit-jupyter) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->streamlit-jupyter) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->streamlit-jupyter) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.19->streamlit-jupyter) (0.28.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->streamlit-jupyter) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter->streamlit-jupyter) (4.5.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (1.9.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter->streamlit-jupyter) (25.1.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->streamlit-jupyter) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->streamlit-jupyter)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert->jupyter->streamlit-jupyter) (2.21.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8->streamlit-jupyter) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.19->streamlit-jupyter) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter->streamlit-jupyter) (2.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter->streamlit-jupyter) (1.3.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (0.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->streamlit-jupyter) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->streamlit-jupyter) (2.23)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->streamlit-jupyter) (1.4.0)\n",
            "Downloading streamlit_jupyter-0.3.1-py3-none-any.whl (13 kB)\n",
            "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading stqdm-0.0.5-py3-none-any.whl (11 kB)\n",
            "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.4.10-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: widgetsnbextension, json5, jedi, comm, async-lru, ipywidgets, stqdm, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, streamlit-jupyter\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed async-lru-2.0.5 comm-0.2.3 ipywidgets-8.1.8 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.4.10 jupyterlab-server-2.28.0 stqdm-0.0.5 streamlit-jupyter-0.3.1 widgetsnbextension-4.0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from streamlit_jupyter import StreamlitPatcher\n",
        "StreamlitPatcher().jupyter()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pickle"
      ],
      "metadata": {
        "id": "33nBcSlaB4Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5659d5d6",
        "outputId": "2a367def-ea7a-4ded-b572-086f1e13dc84"
      },
      "source": [
        "!pip show streamlit-jupyter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: streamlit-jupyter\n",
            "Version: 0.3.1\n",
            "Summary: Simple Python package to preview and develop streamlit apps in jupyter notebooks\n",
            "Home-page: https://github.com/ddobrinskiy/streamlit-jupyter\n",
            "Author: David Dobrinskiy\n",
            "Author-email: david@dobrinskiy.me\n",
            "License: Apache Software License 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: fastcore, ipywidgets, jupyter, stqdm, streamlit, tabulate, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from google.colab import output\n",
        "output.enable_custom_widget_manager()'''"
      ],
      "metadata": {
        "id": "QWvfOLrqGC87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS CODE DOESN'T RUN BECAUSE OF THE WAY STREAMLIT REACTS WITH COLAB NOTEBOOK\n",
        "'''st.title(' Spam Classifier ðŸ“§')\n",
        "user_input = st.text_area('Email:', height=200)\n",
        "if st.button('Classify'):\n",
        "    if user_input:\n",
        "        input_vec = vectorizer.transform([user_input.lower()])\n",
        "        pred = model1.predict(input_vec)[0]\n",
        "        if pred == 1:\n",
        "            st.error(' SPAM ðŸš«')\n",
        "        else:\n",
        "            st.success(' HAM âœ…')\n",
        "    else:\n",
        "        st.warning(' Enter text')\n",
        "st.caption('Spam Classifier')'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "uoApusdtCJQE",
        "outputId": "26221645-a8e1-4e6c-8340-fe8ddff324a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"st.title(' Spam Classifier ðŸ“§')\\nuser_input = st.text_area('Email:', height=200)\\nif st.button('Classify'):\\n    if user_input:\\n        input_vec = vectorizer.transform([user_input.lower()])\\n        pred = model1.predict(input_vec)[0]\\n        if pred == 1:\\n            st.error(' SPAM ðŸš«')\\n        else:\\n            st.success(' HAM âœ…')\\n    else:\\n        st.warning(' Enter text')\\nst.caption('Spam Classifier')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "62097690",
        "outputId": "6d4952ca-5900-480b-d43b-ffa9aa3d9913"
      },
      "source": [
        "# Example text to classify\n",
        "test_email = \"Congratulations! You have won a $1000 gift card. Click here to claim your prize now. Limited time offer, act fast!\"\n",
        "\n",
        "# Preprocess the text (lowercase, tokenize, remove punctuation and stopwords, lemmatize)\n",
        "# We are reusing the steps of preprocessing we defined earlier\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt', quiet=True) #quiet helps making downloading unseen\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [tok for tok in tokens if tok not in string.punctuation]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "processed_email = preprocess_text(test_email)\n",
        "print(f\"Processed email: {processed_email}\")\n",
        "\n",
        "# Vectorize the processed text using the fitted vectorizer\n",
        "input_vec = vectorizer.transform([processed_email])\n",
        "\n",
        "# Predict the class\n",
        "prediction = model1.predict(input_vec)[0]\n",
        "\n",
        "# Print the prediction\n",
        "if prediction == 1:\n",
        "    print(\"Prediction: SPAM ðŸš«\")\n",
        "else:\n",
        "    print(\"Prediction: HAM âœ…\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed email: congratulation 1000 gift card click claim prize limit time offer act fast\n",
            "Prediction: SPAM ðŸš«\n"
          ]
        }
      ]
    }
  ]
}